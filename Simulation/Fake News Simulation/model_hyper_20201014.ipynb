{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent-based model for News diffusion and misinformation\n",
    "\n",
    "What I want to do is to create agents that share news, then from there find some interesting insights that could help fake news research    \n",
    "Test 1 Does having fake news affect real news sharing?    \n",
    "Test 2 Does a news source matter in the design?    \n",
    "Test 3 Does a news event matter in the design?    \n",
    "Test 4 Do people just share what they are told (Illusory of truth)?    \n",
    "Test 5 How people could be silenced?    \n",
    "Test 6 Does fake news shift the public opinion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first thing we need to do is to import news article and user classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from SimHelper import generate_weighted_sentiment\n",
    "from random import sample, randint\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from classes import NewsArticle,User, NewsAgency\n",
    "import pylab\n",
    "import networkx as nx\n",
    "from random import random, uniform, choice\n",
    "from SimHelper import generate_weighted_percentage, generate_bias\n",
    "from scipy.stats import beta\n",
    "from random import sample\n",
    "import collections\n",
    "import scipy.stats\n",
    "import itertools\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_source_to_list(source):\n",
    "    y = [0]*number_of_news_users\n",
    "    y[source] = 1\n",
    "    return y\n",
    "\n",
    "def create_news_event (user,sentiment,fake,tick):\n",
    "    \"\"\"\n",
    "    Make a news event\n",
    "    a. news_article_id\n",
    "    b. source_id : the id of the source\n",
    "    c. source_preference : the preference of the source\n",
    "    d. sentiment : a number from [1,5] or [-5,-1]\n",
    "    e. num_shares : the number of people sharing the article\n",
    "    f. fake : 1 if the news is fake, otherwise real\n",
    "    \"\"\"\n",
    "    # assign an id in the format of user id + 1000\n",
    "    if not user.articles:\n",
    "        news_id = int(str(user.unique_id)+str(1000))\n",
    "    else:\n",
    "        news_id = user.articles[-1].news_article_id +1\n",
    "#     print(news_id)\n",
    "    # get source_id and source_preference\n",
    "    source_id = user.unique_id\n",
    "    source_preference = user.preference\n",
    "    #generate a preference randomly distributed around the source preference\n",
    "    if fake:\n",
    "        article_preference = round(np.random.normal(loc=source_preference, scale = 0.14),2)\n",
    "        if np.sign(article_preference) != np.sign(user.preference):\n",
    "            article_preference *= -1\n",
    "    else:\n",
    "        article_preference = round(np.random.normal(loc=source_preference, scale = 0.07),2)\n",
    "    if article_preference > 1:\n",
    "        article_preference = 1\n",
    "    elif article_preference < -1:\n",
    "        article_preference = -1\n",
    "    sentiment = sentiment\n",
    "    num_shares = 1\n",
    "    fake = fake\n",
    "    tick = tick\n",
    "    \n",
    "    article = NewsArticle(news_id,source_id,source_preference,article_preference,sentiment,num_shares,fake,tick)\n",
    "    \n",
    "    user.articles.append(article)\n",
    "\n",
    "def get_similarity(x,y):\n",
    "    return round(1 - abs(G.nodes[x]['preference'] - G.nodes[y]['preference'])/2,2)\n",
    "\n",
    "def get_neighbor_preferences(user):\n",
    "    \"\"\"\n",
    "    Get the average preference of a user's neighbors\n",
    "    \"\"\"\n",
    "    return round(np.mean([G.nodes[n]['preference'] for n in list(G[user.unique_id])]),2)\n",
    "#     return round(np.mean([o.preference for o in list(G[users[user.unique_id]])]),2)\n",
    "\n",
    "    # Simpler version\n",
    "#     temp = []\n",
    "#     for n in list(G[users[user.unique_id]]):\n",
    "#         print(abs(user.preference - n.preference))\n",
    "#         temp.append(n.preference)\n",
    "#     return round(np.mean(temp),2)\n",
    "\n",
    "def get_recommendations(user,tick):\n",
    "    \"\"\"\n",
    "    Pick what the user will see on their feed according to a platform strategy\n",
    "    \n",
    "    Strategy 1: See the latest news from your neighbors\n",
    "    Strategy 2: See the most popular news\n",
    "    Strategy 3: See the most important news in your circle\n",
    "    Strategy 4: See a random selected news as a benchmark\n",
    "    Strategy 5: Weighted approach\n",
    "    Strategy 6: Most shocking news\n",
    "    \"\"\"\n",
    "    recommendations = [article for neighbor in list(G[user.unique_id]) for article in G.nodes[neighbor]['articles'] if article.tick > tick -5]\n",
    "    recommendations = [article for article in recommendations if np.sign(user.preference) == np.sign(article.article_preference)]\n",
    "    # try to not recommend articles that look like fake news\n",
    "#     print(article_sentiments_mean)\n",
    "#     recommendations = [article for article in recommendations if (abs(article.sentiment) < sentiment_threshold and article.num_shares > int(num_shares_n))or random()>0.5]\n",
    "#     recommendations.sort(key=lambda x: (x.tick,abs(x.sentiment)), reverse=True)\n",
    "\n",
    "    #using a fake news classifier\n",
    "    if (tick > 0) and (tick/pause > pause/tick) :\n",
    "        for rec in recommendations:\n",
    "            if clf.predict([[rec.sentiment,rec.num_shares]+convert_source_to_list(rec.source_id)])[0] == 1:\n",
    "                    recommendations.remove(rec)\n",
    "    \n",
    "    try:\n",
    "        return recommendations[:3]\n",
    "    except:\n",
    "        return recommendations\n",
    "\n",
    "def spread_news(user,tick):\n",
    "    \"\"\"\n",
    "    A user will share to their neighbors based on preference, sentiment, and number of shares\n",
    "    \n",
    "    In terms of preference, there are a few measures to find:\n",
    "    1- individual preferences: user preference and neighbor preference\n",
    "    2- community preference : mean of preference of neighbors of the neighbor\n",
    "    3- \n",
    "    \"\"\"\n",
    "#     if not user.articles:\n",
    "#         return\n",
    "    \n",
    "    community_opinion = get_neighbor_preferences(user)\n",
    "    recommendations = get_recommendations(user,tick)\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        if rec.fake and (random() < accept_fact_checking):\n",
    "            #user caught the fact checking signal\n",
    "            continue\n",
    "        # person likes to share\n",
    "        if generate_weighted_percentage(user.news_spread_chance):\n",
    "            #person is not silenced\n",
    "            if np.sign(community_opinion) == np.sign(user.preference) == np.sign(rec.article_preference):\n",
    "                if rec not in user.articles and user.user_type =='regular':\n",
    "                    user.articles.append(rec)\n",
    "                    spreaders.loc[len(spreaders.index)] = [tick, user.unique_id,rec.source_id,rec.sentiment, rec.num_shares, rec.fake]\n",
    "                    rec.num_shares += 1\n",
    "                    preference_shift = 0.03\n",
    "#                     print(user.unique_id,'before update:',users[user.unique_id].preference)\n",
    "                    users[user.unique_id].preference = round(users[user.unique_id].preference,2)\n",
    "                    if (np.sign(user.preference) > 0):\n",
    "                        if users[user.unique_id].preference > rec.article_preference:\n",
    "                            users[user.unique_id].preference -= preference_shift\n",
    "#                             print(user.unique_id,'after update:',users[user.unique_id].preference)\n",
    "                        elif users[user.unique_id].preference < rec.article_preference:\n",
    "                            users[user.unique_id].preference += preference_shift\n",
    "#                             print(user.unique_id,'after update:',users[user.unique_id].preference)\n",
    "                    elif (np.sign(user.preference) < 0):\n",
    "                        if users[user.unique_id].preference > rec.article_preference:\n",
    "                            users[user.unique_id].preference -= preference_shift\n",
    "#                             print(user.unique_id,'after update:',users[user.unique_id].preference)\n",
    "                        elif users[user.unique_id].preference < rec.article_preference:\n",
    "                            users[user.unique_id].preference += preference_shift\n",
    "#                             print(user.unique_id,'after update:',users[user.unique_id].preference)\n",
    "                    if users[user.unique_id].preference > 1:\n",
    "                        users[user.unique_id].preference = 1\n",
    "                    elif users[user.unique_id].preference < -1:\n",
    "                        users[user.unique_id].preference = -1\n",
    "\n",
    "def get_regular_user_preferences():\n",
    "    \"\"\"\n",
    "    return the mean and std dev of the user preference\n",
    "    \"\"\"\n",
    "#     preferences = []\n",
    "#     for node in regular_users:\n",
    "#         preferences.append(node.preference)\n",
    "\n",
    "    preferences = []\n",
    "    for node in users:\n",
    "        preferences.append(node.preference)\n",
    "\n",
    "    print('mean = ',round(np.mean(preferences),2),'standard dev = ',round(np.std(preferences),2))\n",
    "    plt.hist(preferences)\n",
    "\n",
    "def clear_all_news():\n",
    "    [user.articles.clear() for user in users]\n",
    "    \n",
    "def train_fake_news_classifier(verbose=0):\n",
    "#     df = pd.get_dummies(spreaders,columns=['source'])\n",
    "    df = spreaders.copy()\n",
    "    sources_df = []\n",
    "    for i in range(number_of_news_users):\n",
    "        sources_df.append('source_'+str(i))\n",
    "    sources_df\n",
    "\n",
    "    df = pd.concat([df,pd.DataFrame(columns=sources_df)],sort=False)\n",
    "    df.fillna(0,inplace=True)\n",
    "    for index,row in df.iterrows():\n",
    "        column_name = 'source_'+str(int(row['source']))\n",
    "        df.loc[index,column_name]  = 1\n",
    "    df.drop(columns=['source'],inplace=True)\n",
    "    df = df[[c for c in df if c not in ['fake']]+['fake']]\n",
    "    clf = svm.SVC(gamma='scale')\n",
    "    clf.fit(df.drop(columns=['tick','user','fake']), df['fake'])\n",
    "#     print(df.drop(columns=['tick','user','fake']))\n",
    "    if verbose >= 1:\n",
    "        y_pred = clf.predict(df.drop(columns=['tick','user','fake']))\n",
    "        print('MSE',mean_squared_error(df['fake'].to_numpy(),y_pred))\n",
    "        if verbose == 2:\n",
    "            print(confusion_matrix(df['fake'].to_numpy(),y_pred))\n",
    "            print(classification_report(df['fake'].to_numpy(),y_pred))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spreaders.copy()\n",
    "# sources_df = []\n",
    "# for i in range(number_of_news_users):\n",
    "#     sources_df.append('source_'+str(i))\n",
    "# sources_df\n",
    "\n",
    "# df = pd.concat([df,pd.DataFrame(columns=sources_df)],sort=False)\n",
    "# df.fillna(0,inplace=True)\n",
    "# for index,row in df.iterrows():\n",
    "#     column_name = 'source_'+str(int(row['source']))\n",
    "#     df.loc[index,column_name]  = 1\n",
    "# df.drop(columns=['source'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. User attributes:  \n",
    "  a. unique_id : a non-intelligent number given to the user  \n",
    "  b. news_spread_chance : a percentage of the user spreading news (can be compared to confidence and censorship)   \n",
    "  c. preference : the position the user feels about the current subject being discussed  \n",
    "  d. user_type: the user could be a regular user or a news agency  \n",
    "  e. articles : a list of articles shared by the user  \n",
    "\n",
    "\n",
    "\n",
    "2. News agency attributes:  \n",
    "  a. unique_id : a non-intelligent number given to the user  \n",
    "  b. news_spread_chance : a percentage of the user spreading news (can be compared to confidence and censorship)   \n",
    "  c. preference : the position the user feels about the current subject being discussed  \n",
    "  d. user_type: the user could be a regular user or a news agency  \n",
    "  e. articles : a list of articles shared by the user  \n",
    "  f. reliable : indicates how reliable this agency is\n",
    "        \n",
    "        \n",
    "3. News article attributes:  \n",
    "  a. news_article_id : a non-intelligenct number given to the article  \n",
    "  b. source_id : the id of the source  \n",
    "  c. source_preference : the preference of the source  \n",
    "  d. sentiment : a number from [1,5] or [-5,-1]  \n",
    "  e. num_shares :  the number of people sharing the article  \n",
    "  f. fake : 1 if the news is fake, otherwise real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets try to create a simple news article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we need to try to create a few users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "news_agencies_ratio is a network attribute to add that takes the amount of news agencies in the network  \n",
    "number_of_users is a network attribute that creates a graph of a certain number of nodes equal to the number of users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try to create a network of the users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add edges by Power law distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets spread some news and see how users change their preferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ticks is another network variable that is used to measure time, it does not mean a specfic time stamp we know at the moment  \n",
    "fake_news_prob is another network variable that measures how much the percentage of fake news will be generated throughout the network  \n",
    "For every time step t:  \n",
    "- Make news events from the news agencies  \n",
    "- Ask every agent to spread their articles across the network  \n",
    "- Change the 'regular' user preference and compute overall network preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of classes has to be greater than one; got 1 class",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f7a8594debca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;31m# train the classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtick\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m                 \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fake_news_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;31m#             print('mean = ',round(np.mean(preferences),2),'standard dev = ',round(np.std(preferences),2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-e632f2ca4c74>\u001b[0m in \u001b[0;36mtrain_fake_news_classifier\u001b[1;34m(verbose)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'fake'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fake'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'scale'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tick'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'user'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'fake'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fake'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;31m#     print(df.drop(columns=['tick','user','fake']))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\simulations\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    148\u001b[0m                          \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                          accept_large_sparse=False)\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         sample_weight = np.asarray([]\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\simulations\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_validate_targets\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    523\u001b[0m             raise ValueError(\n\u001b[0;32m    524\u001b[0m                 \u001b[1;34m\"The number of classes has to be greater than one; got %d\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m                 \" class\" % len(cls))\n\u001b[0m\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The number of classes has to be greater than one; got 1 class"
     ]
    }
   ],
   "source": [
    "number_of_users = 100\n",
    "news_agencies_ratio = 0.1\n",
    "news_reliability = 0.1\n",
    "ticks = 100\n",
    "pause = int(0.2 * ticks)\n",
    "fake_news_prob = 0.5\n",
    "accept_fact_checking = 0.999\n",
    "sentiment_threshold = 4.1\n",
    "num_shares_n= 3\n",
    "\n",
    "mu = {new_list: [] for new_list in range(0,ticks,pause)} \n",
    "si = {new_list: [] for new_list in range(0,ticks,pause)} \n",
    "fk = []\n",
    "rl= []\n",
    "all_news = []\n",
    "\n",
    "for z in range(50):\n",
    "    \n",
    "    users = []\n",
    "    regular_users = []\n",
    "    news_users = []\n",
    "    \n",
    "\n",
    "\n",
    "    number_of_regualr_users = int(number_of_users*(1-news_agencies_ratio))\n",
    "    number_of_news_users = number_of_users - number_of_regualr_users\n",
    "\n",
    "    \n",
    "    #create the distribution for news agencies to be two concatenatened beta distribution with alpha=beta=3, one of them shifted by -1\n",
    "    data_beta_neg = beta.rvs(a=3,b=3,size=1000)\n",
    "    data_beta_neg = [x-1 for x in data_beta_neg]\n",
    "    \n",
    "    data_beta_pos = list(beta.rvs(a=3,b=3,size=1000))\n",
    "    \n",
    "    beta_dist = data_beta_neg + data_beta_pos\n",
    "\n",
    "\n",
    "    for i in range(number_of_news_users):\n",
    "        unique_id = i\n",
    "        user_type = 'news_agency' \n",
    "        news_spread_chance = 1\n",
    "        articles = []\n",
    "        reliable = (generate_weighted_percentage(news_reliability))\n",
    "        if (reliable == 0) and (random() >0.5):\n",
    "            preference = round(choice([x for x in beta_dist if abs(x)>0.4]),2)\n",
    "        else:\n",
    "            preference = round(choice([x for x in beta_dist if abs(x)<0.5]),2)\n",
    "        news_users.append(NewsAgency(unique_id,news_spread_chance,preference,user_type,articles,reliable))\n",
    "        users.append(NewsAgency(unique_id,news_spread_chance,preference,user_type,articles,reliable))\n",
    "\n",
    "    for i in range(len(news_users),number_of_regualr_users+len(news_users)):\n",
    "        unique_id = i\n",
    "        user_type = 'regular'\n",
    "        news_spread_chance = round(random(),2)\n",
    "        preference = generate_bias(mu = 0, sigma = 3)    \n",
    "        articles = []\n",
    "        regular_users.append(User(unique_id,news_spread_chance,preference,user_type,articles))\n",
    "        users.append(User(unique_id,news_spread_chance,preference,user_type,articles))\n",
    "\n",
    "    G = nx.Graph()\n",
    "    color_map = []\n",
    "    for u in users:    \n",
    "        if isinstance(u,NewsAgency):\n",
    "            G.add_node(u.unique_id,s=\"^\", news_spread_chance = u.news_spread_chance, \n",
    "                       preference = u.preference, reliable= u.reliable, articles = u.articles, user_type = u.user_type )\n",
    "        else:\n",
    "            G.add_node(u.unique_id,s=\"o\", news_spread_chance = u.news_spread_chance, \n",
    "                       preference = u.preference, articles = u.articles, user_type = u.user_type )\n",
    "        if u.preference < -0.2:\n",
    "            color_map.append('red')\n",
    "        elif u.preference > 0.2:\n",
    "            color_map.append('blue')\n",
    "        else:\n",
    "            color_map.append('grey') \n",
    "            \n",
    "            \n",
    "    edge_list = []\n",
    "    for i in range(number_of_users):\n",
    "        number_of_edges = int((np.random.pareto(a=5)+.1)*30)\n",
    "        if number_of_edges > number_of_users:\n",
    "            number_of_edges /= 2\n",
    "        edge_list.append(int(number_of_edges))\n",
    "        \n",
    "    \n",
    "    for i in range(number_of_users):\n",
    "        # I am writing the sample function in while True loop to make sure it happens\n",
    "        while True:\n",
    "            try:\n",
    "                neighbors = sample(users,edge_list[i])\n",
    "            except:\n",
    "                continue\n",
    "            break\n",
    "    #     print(neighbors)\n",
    "        for neighbor in neighbors:\n",
    "            u = users[i].unique_id\n",
    "            v = neighbor.unique_id\n",
    "            s = get_similarity(u,v)\n",
    "            G.add_edge(u,v,weight=s)  \n",
    "            \n",
    "    clear_all_news()\n",
    "\n",
    "#     fig,axs = plt.subplots(int(ticks/pause),figsize=(25,25))\n",
    "#     fig.suptitle('Regular users preferences over time')\n",
    "#     plots = 0\n",
    "    spreaders = pd.DataFrame(columns=['tick','user','source','sentiment','num_shares','fake'])\n",
    "    for tick in range(ticks):\n",
    "        if tick % pause == 0:\n",
    "#             print('tick number: ',tick)\n",
    "    #         get_regular_user_preferences()\n",
    "            preferences = [user.preference for user in users if user.user_type == 'regular']\n",
    "            mu[tick].append(np.mean(preferences))\n",
    "            si[tick].append(np.std(preferences))\n",
    "            # train the classifier\n",
    "            if tick>0:\n",
    "                clf = train_fake_news_classifier()\n",
    "#             print('mean = ',round(np.mean(preferences),2),'standard dev = ',round(np.std(preferences),2))\n",
    "            \n",
    "#             axs[plots].hist(preferences)\n",
    "#             plots += 1\n",
    "\n",
    "        sampled_news_agencies = sample(news_users,randint(1,len(news_users)-1))\n",
    "        for n in sampled_news_agencies:\n",
    "            sentiment = generate_weighted_sentiment()\n",
    "            fake = 0\n",
    "            if n.reliable == 0:\n",
    "                if generate_weighted_percentage(fake_news_prob):\n",
    "                    #generate fake news\n",
    "                    sentiment = generate_weighted_sentiment(a=4)\n",
    "                    fake = 1\n",
    "            create_news_event(n,sentiment,fake,tick)\n",
    "            spread_news(n,tick)\n",
    "        sampled_regular_users = sample(regular_users,randint(1,len(regular_users)-1))\n",
    "        for u in sampled_regular_users:\n",
    "            spread_news(u,tick)\n",
    "        \n",
    "    ## store news in a df to identify spammers    \n",
    "#     print('news spreaders',len(spreaders))\n",
    "#     print(spreaders.head())\n",
    "            \n",
    "    w = []\n",
    "    for u in users:\n",
    "        if u.articles:\n",
    "            for a in u.articles:\n",
    "                w.append(a)\n",
    "\n",
    "    w.sort(key=lambda x: x.num_shares, reverse=True)\n",
    "\n",
    "    getx, gety = lambda a: a.num_shares, lambda a: a.news_article_id # or use operator.itemgetter\n",
    "    groups = itertools.groupby(sorted(w, key=getx), key=getx)\n",
    "    m = [max(b, key=gety) for a,b in groups]\n",
    "    copy = [l for l in w if l in m]\n",
    "\n",
    "    copy.sort(key=lambda x: x.num_shares, reverse=True)\n",
    "    \n",
    "    \n",
    "    all_news = all_news + copy\n",
    "    \n",
    "    fk.append(sum([x.num_shares for x in copy if x.fake]))\n",
    "\n",
    "    rl.append(sum([x.num_shares for x in copy if not x.fake]))\n",
    "    print('finished round:',z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shares_fake=[x.num_shares for x in all_news if x.fake == 1]\n",
    "shares_real=[x.num_shares for x in all_news if x.fake == 0]\n",
    "\n",
    "print(sum(shares_fake)/(sum(shares_fake)+sum(shares_real)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(shares_real,alpha = 0.5, color = 'green',label='Real')\n",
    "plt.hist(shares_fake,alpha = 0.7, color = 'red',label='Fake')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('News sharing results with a fake news classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished round: 0\n",
      "finished round: 1\n",
      "finished round: 2\n",
      "finished round: 3\n",
      "finished round: 4\n",
      "finished round: 5\n",
      "finished round: 6\n",
      "finished round: 7\n",
      "finished round: 8\n",
      "finished round: 9\n",
      "finished round: 10\n",
      "finished round: 11\n",
      "finished round: 12\n",
      "finished round: 13\n",
      "finished round: 14\n",
      "finished round: 15\n",
      "finished round: 16\n",
      "finished round: 17\n",
      "finished round: 18\n",
      "finished round: 19\n",
      "finished round: 20\n",
      "finished round: 21\n",
      "finished round: 22\n",
      "finished round: 23\n",
      "finished round: 24\n",
      "finished round: 25\n",
      "finished round: 26\n",
      "finished round: 27\n",
      "finished round: 28\n",
      "finished round: 29\n",
      "finished round: 30\n",
      "finished round: 31\n",
      "finished round: 32\n",
      "finished round: 33\n",
      "finished round: 34\n",
      "finished round: 35\n",
      "finished round: 36\n",
      "finished round: 37\n",
      "finished round: 38\n",
      "finished round: 39\n",
      "finished round: 40\n",
      "finished round: 41\n",
      "finished round: 42\n",
      "finished round: 43\n",
      "finished round: 44\n",
      "finished round: 45\n",
      "finished round: 46\n",
      "finished round: 47\n",
      "finished round: 48\n",
      "finished round: 49\n",
      "4.9 3 : 0.5346425237493823\n",
      "finished round: 0\n",
      "finished round: 1\n",
      "finished round: 2\n",
      "finished round: 3\n",
      "finished round: 4\n",
      "finished round: 5\n",
      "finished round: 6\n",
      "finished round: 7\n",
      "finished round: 8\n",
      "finished round: 9\n",
      "finished round: 10\n",
      "finished round: 11\n"
     ]
    }
   ],
   "source": [
    "fake_news_percentage = {}\n",
    "for senti_t in range(49,40,-2):\n",
    "    for num_shares_n in range(3,16,3):\n",
    "        number_of_users = 100\n",
    "        news_agencies_ratio = 0.1\n",
    "        news_reliability = 0.1\n",
    "        ticks = 100\n",
    "        pause = int(0.2 * ticks)\n",
    "        fake_news_prob = 0.5\n",
    "        accept_fact_checking = 1\n",
    "        sentiment_threshold = senti_t/10\n",
    "        num_shares_threshold = num_shares_n\n",
    "\n",
    "        mu = {new_list: [] for new_list in range(0,ticks,pause)} \n",
    "        si = {new_list: [] for new_list in range(0,ticks,pause)} \n",
    "        fk = []\n",
    "        rl= []\n",
    "        all_news = []\n",
    "\n",
    "        for z in range(50):\n",
    "\n",
    "            users = []\n",
    "            regular_users = []\n",
    "            news_users = []\n",
    "\n",
    "            number_of_regualr_users = int(number_of_users*(1-news_agencies_ratio))\n",
    "            number_of_news_users = number_of_users - number_of_regualr_users\n",
    "\n",
    "\n",
    "            #create the distribution for news agencies to be two concatenatened beta distribution with alpha=beta=3, one of them shifted by -1\n",
    "            data_beta_neg = beta.rvs(a=3,b=3,size=1000)\n",
    "            data_beta_neg = [x-1 for x in data_beta_neg]\n",
    "\n",
    "            data_beta_pos = list(beta.rvs(a=3,b=3,size=1000))\n",
    "\n",
    "            beta_dist = data_beta_neg + data_beta_pos\n",
    "\n",
    "\n",
    "            for i in range(number_of_news_users):\n",
    "                unique_id = i\n",
    "                user_type = 'news_agency' \n",
    "                news_spread_chance = 1\n",
    "                articles = []\n",
    "                reliable = (generate_weighted_percentage(news_reliability))\n",
    "                if (reliable == 0) and (random() >0.5):\n",
    "                    preference = round(choice([x for x in beta_dist if abs(x)>0.4]),2)\n",
    "                else:\n",
    "                    preference = round(choice([x for x in beta_dist if abs(x)<0.5]),2)\n",
    "                news_users.append(NewsAgency(unique_id,news_spread_chance,preference,user_type,articles,reliable))\n",
    "                users.append(NewsAgency(unique_id,news_spread_chance,preference,user_type,articles,reliable))\n",
    "\n",
    "            for i in range(len(news_users),number_of_regualr_users+len(news_users)):\n",
    "                unique_id = i\n",
    "                user_type = 'regular'\n",
    "                news_spread_chance = round(random(),2)\n",
    "                preference = generate_bias(mu = 0, sigma = 3)    \n",
    "                articles = []\n",
    "                regular_users.append(User(unique_id,news_spread_chance,preference,user_type,articles))\n",
    "                users.append(User(unique_id,news_spread_chance,preference,user_type,articles))\n",
    "\n",
    "            G = nx.Graph()\n",
    "            color_map = []\n",
    "            for u in users:    \n",
    "                if isinstance(u,NewsAgency):\n",
    "                    G.add_node(u.unique_id,s=\"^\", news_spread_chance = u.news_spread_chance, \n",
    "                               preference = u.preference, reliable= u.reliable, articles = u.articles, user_type = u.user_type )\n",
    "                else:\n",
    "                    G.add_node(u.unique_id,s=\"o\", news_spread_chance = u.news_spread_chance, \n",
    "                               preference = u.preference, articles = u.articles, user_type = u.user_type )\n",
    "                if u.preference < -0.2:\n",
    "                    color_map.append('red')\n",
    "                elif u.preference > 0.2:\n",
    "                    color_map.append('blue')\n",
    "                else:\n",
    "                    color_map.append('grey') \n",
    "\n",
    "\n",
    "            edge_list = []\n",
    "            for i in range(number_of_users):\n",
    "                number_of_edges = int((np.random.pareto(a=5)+.1)*30)\n",
    "                if number_of_edges > number_of_users:\n",
    "                    number_of_edges /= 2\n",
    "                edge_list.append(int(number_of_edges))\n",
    "\n",
    "\n",
    "            for i in range(number_of_users):\n",
    "                # I am writing the sample function in while True loop to make sure it happens\n",
    "                while True:\n",
    "                    try:\n",
    "                        neighbors = sample(users,edge_list[i])\n",
    "                    except:\n",
    "                        continue\n",
    "                    break\n",
    "            #     print(neighbors)\n",
    "                for neighbor in neighbors:\n",
    "                    u = users[i].unique_id\n",
    "                    v = neighbor.unique_id\n",
    "                    s = get_similarity(u,v)\n",
    "                    G.add_edge(u,v,weight=s)  \n",
    "\n",
    "            clear_all_news()\n",
    "\n",
    "        #     fig,axs = plt.subplots(int(ticks/pause),figsize=(25,25))\n",
    "        #     fig.suptitle('Regular users preferences over time')\n",
    "        #     plots = 0\n",
    "            spreaders = pd.DataFrame(columns=['tick','user','source','sentiment','num_shares','fake'])\n",
    "            for tick in range(ticks):\n",
    "                if tick % pause == 0:\n",
    "        #             print('tick number: ',tick)\n",
    "            #         get_regular_user_preferences()\n",
    "                    preferences = [user.preference for user in users if user.user_type == 'regular']\n",
    "                    mu[tick].append(np.mean(preferences))\n",
    "                    si[tick].append(np.std(preferences))\n",
    "                    # train the classifier\n",
    "    #                 if tick>0:\n",
    "    #                     clf = train_fake_news_classifier()\n",
    "        #             print('mean = ',round(np.mean(preferences),2),'standard dev = ',round(np.std(preferences),2))\n",
    "\n",
    "        #             axs[plots].hist(preferences)\n",
    "        #             plots += 1\n",
    "\n",
    "                sampled_news_agencies = sample(news_users,randint(1,len(news_users)-1))\n",
    "                for n in sampled_news_agencies:\n",
    "                    sentiment = generate_weighted_sentiment()\n",
    "                    fake = 0\n",
    "                    if n.reliable == 0:\n",
    "                        if generate_weighted_percentage(fake_news_prob):\n",
    "                            #generate fake news\n",
    "                            sentiment = generate_weighted_sentiment(a=4)\n",
    "                            fake = 1\n",
    "                    create_news_event(n,sentiment,fake,tick)\n",
    "                    spread_news(n,tick)\n",
    "                sampled_regular_users = sample(regular_users,randint(1,len(regular_users)-1))\n",
    "                for u in sampled_regular_users:\n",
    "                    spread_news(u,tick)\n",
    "\n",
    "            ## store news in a df to identify spammers    \n",
    "        #     print('news spreaders',len(spreaders))\n",
    "        #     print(spreaders.head())\n",
    "\n",
    "            w = []\n",
    "            for u in users:\n",
    "                if u.articles:\n",
    "                    for a in u.articles:\n",
    "                        w.append(a)\n",
    "\n",
    "            w.sort(key=lambda x: x.num_shares, reverse=True)\n",
    "\n",
    "            getx, gety = lambda a: a.num_shares, lambda a: a.news_article_id # or use operator.itemgetter\n",
    "            groups = itertools.groupby(sorted(w, key=getx), key=getx)\n",
    "            m = [max(b, key=gety) for a,b in groups]\n",
    "            copy = [l for l in w if l in m]\n",
    "\n",
    "            copy.sort(key=lambda x: x.num_shares, reverse=True)\n",
    "\n",
    "\n",
    "            all_news = all_news + copy\n",
    "\n",
    "            fk.append(sum([x.num_shares for x in copy if x.fake]))\n",
    "\n",
    "            rl.append(sum([x.num_shares for x in copy if not x.fake]))\n",
    "            print('finished round:',z)\n",
    "        shares_fake=[x.num_shares for x in all_news if x.fake == 1]\n",
    "        shares_real=[x.num_shares for x in all_news if x.fake == 0]\n",
    "        fn_percentage = sum(shares_fake)/(sum(shares_fake)+sum(shares_real))\n",
    "        fake_news_percentage[(sentiment_threshold,num_shares_n)] = fn_percentage\n",
    "        print(sentiment_threshold,num_shares_n,\":\",fn_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4.9: 0.5483019594135505, 4.5: 0.5558534796931259, 4.1: 0.5113095890410959}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_news_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.007855555555555555 -0.007722222222222222 -0.0021888888888888874 -0.0050555555555555545 -0.010322222222222222\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(mu[0]),np.mean(mu[100]),np.mean(mu[200]),np.mean(mu[300]),np.mean(mu[400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.298053701786515 0.5666194816981419 0.5883223133197757 0.5887259524968597 0.5938585493413071\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(si[0]),np.mean(si[100]),np.mean(si[200]),np.mean(si[300]),np.mean(si[400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.298053701786515, 0.2920978124635251, 0.30400959110950493)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_confidence_interval(si[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4595.333333333333"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(fk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2780.9583333333335"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6229869344916992"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(fk)/(np.mean(fk)+np.mean(rl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda-simulations]",
   "language": "python",
   "name": "conda-env-conda-simulations-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
